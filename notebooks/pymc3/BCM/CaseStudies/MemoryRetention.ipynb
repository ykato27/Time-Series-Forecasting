{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import theano\n",
    "\n",
    "from scipy import stats\n",
    "from theano import tensor as tt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "warnings.simplefilter(action=\"ignore\", category=(FutureWarning, UserWarning))\n",
    "RANDOM_SEED = 8927\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.style.use(\"arviz-darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10 - Memory retention\n",
    "  \n",
    "This Chapter is about estimating the relationship between memory retention and time.\n",
    "The model being considered is a simplified version of the exponential decay model. The model assumes that the probability that an item will be remembered after a period of time $t$ has elapsed is $\\theta_{t} = \\text{exp}(−\\alpha t)+\\beta$, with the restriction $0 < \\theta_{t} < 1$. The $\\alpha$ parameter corresponds to the rate of decay of information. The $\\beta$ parameter corresponds to a baseline level of remembering that is assumed to remain even after very long time periods.\n",
    "  \n",
    "## 10.1 No individual differences\n",
    "\n",
    "\n",
    "$$ \\alpha \\sim \\text{Beta}(1,1)$$\n",
    "$$ \\beta \\sim \\text{Beta}(1,1)$$\n",
    "$$ \\theta_{j} = \\text{min}(1,\\text{exp}(−\\alpha t_{j})+\\beta)$$\n",
    "$$ k_{ij} \\sim \\text{Binomial}(\\theta_{j},n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to literal (<ipython-input-4-9efceedc199d>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-9efceedc199d>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    1 = np.ma.masked_values([18, 18, 16, 13, 9, 6, 4, 4, 4, -999,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to literal\n"
     ]
    }
   ],
   "source": [
    "t = np.array([1, 2, 4, 7, 12, 21, 35, 59, 99, 200])\n",
    "nt = len(t)\n",
    "# slist = [0,1,2,3]\n",
    "ns = 4\n",
    "tmat = np.repeat(t, ns).reshape(nt, -1).T\n",
    "1 = np.ma.masked_values([18, 18, 16, 13, 9, 6, 4, 4, 4, -999,\n",
    "                          17, 13,  9,  6, 4, 4, 4, 4, 4, -999,\n",
    "                          14, 10,  6,  4, 4, 4, 4, 4, 4, -999,\n",
    "                          -999, -999, -999, -999, -999, -999, -999, -999, -999, -999], \n",
    "                          value=-999).reshape(ns,-1)\n",
    "n = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model1:\n",
    "    # prior\n",
    "    alpha = pm.Beta(\"alpha\", alpha=1, beta=1, testval=0.30)\n",
    "    beta = pm.Beta(\"beta\", alpha=1, beta=1, testval=0.25)\n",
    "\n",
    "    # parameter transformation\n",
    "    theta = tt.exp(-alpha * tmat) + beta\n",
    "    # thetaj = pm.Deterministic('thetaj', tt.clip(theta, 0, 1))\n",
    "    thetaj = pm.Deterministic(\"thetaj\", tt.minimum(theta, 1))\n",
    "\n",
    "    kij = pm.Binomial(\"kij\", p=thetaj, n=n, observed=k1)\n",
    "\n",
    "    step = pm.NUTS(target_accept=0.99)\n",
    "    trace1 = pm.sample(2000, tune=2000, step=step)\n",
    "\n",
    "az.plot_trace(trace1, var_names=[\"alpha\", \"beta\", \"thetaj\"], compact=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model is very sensitive to the starting value. We can specify a starting value for each parameter by assigning a `testval` when the RV is created:\n",
    "```python\n",
    "alpha = pm.Beta('alpha', alpha=1, beta=1, testval=.30)\n",
    "```\n",
    "\n",
    "In fact, with a bad starting value, NUTS really has a hard time sampling, and we get a `Bad initial energy` error. The reason is that bounding the theta gives 0 gradient, which is a problem as NUTS needs th gradient to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproduce Fig 10.2\n",
    "xtrace = trace1[\"alpha\"]\n",
    "ytrace = trace1[\"beta\"]\n",
    "\n",
    "datmp2 = np.vstack((xtrace, ytrace))\n",
    "df2 = pd.DataFrame(datmp2.transpose(), columns=[\"x\", \"y\"])\n",
    "\n",
    "g = sns.jointplot(\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    data=df2,\n",
    "    kind=\"kde\",\n",
    "    color=\"g\",\n",
    "    stat_func=None,\n",
    "    xlim=(0, 1),\n",
    "    ylim=(0, 1),\n",
    ")\n",
    "g.plot_joint(plt.scatter, c=\"g\", s=30, linewidth=1, alpha=0.05)\n",
    "g.ax_joint.collections[0].set_alpha(0)\n",
    "g.set_axis_labels(\"Decay Rate\", \"Baseline\")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproduce Fig 10.3\n",
    "# The posterior predictive distribution for the model that assumes no individual\n",
    "# differences.\n",
    "ppc = pm.sample_posterior_predictive(trace1, model=model1, samples=500)\n",
    "predictrace = ppc[\"kij\"]\n",
    "\n",
    "\n",
    "def plot_predict(predictrace):\n",
    "    ns_, t_ = predictrace.shape[1:]\n",
    "    _, axes = plt.subplots(2, 2, figsize=(16, 16))\n",
    "    bins = np.arange(0, n + 1)\n",
    "    ax = axes.flatten()\n",
    "\n",
    "    for ip in np.arange(ns_):\n",
    "        ispredi = np.squeeze(predictrace[:, ip, :])\n",
    "        ax1 = ax[ip]\n",
    "\n",
    "        for itt in np.arange(t_):\n",
    "            y2, binEdges2 = np.histogram(ispredi[:, itt], bins=bins, normed=True)\n",
    "            ax1.scatter(\n",
    "                itt * np.ones(bins.size), binEdges2 + 1, s=y2 * 1000, c=\"w\", marker=\"s\"\n",
    "            )\n",
    "\n",
    "            if (k1[ip, itt] is np.ma.masked) == 0:\n",
    "                ax1.scatter(\n",
    "                    itt, k1[ip, itt], s=y2[k1[ip, itt] - 1] * 1000, c=\"k\", marker=\"s\"\n",
    "                )\n",
    "\n",
    "        ax1.plot(np.arange(len(t)), k1[ip, :], \"k\", lw=2.5, alpha=0.5)\n",
    "        ax1.set_xlim(-1, 10)\n",
    "        ax1.set_ylim(-1, 19)\n",
    "        ax1.set_title(\"Subject %s\" % (ip + 1))\n",
    "\n",
    "    plt.xticks(np.arange(len(t)), t)\n",
    "    plt.xlabel(\"Time Lags\")\n",
    "    plt.ylabel(\"Retention Count\")\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predict(predictrace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = np.array([1, 2, 4, 7, 12, 21, 35, 59, 99])\n",
    "nt2 = len(t2)\n",
    "# slist = [0,1,2,3]\n",
    "ns2 = 3\n",
    "tmat2 = np.repeat(t2, ns2).reshape(nt2, -1).T\n",
    "k2 = np.asarray([18, 18, 16, 13, 9, 6, 4, 4, 4, \n",
    "                 17, 13,  9,  6, 4, 4, 4, 4, 4, \n",
    "                 14, 10,  6,  4, 4, 4, 4, 4, 4]).reshape(ns2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model1_:\n",
    "    # prior\n",
    "    alpha = pm.Beta(\"alpha\", alpha=1, beta=1)\n",
    "    beta = pm.Beta(\"beta\", alpha=1, beta=1)\n",
    "\n",
    "    # parameter transformation\n",
    "    theta = tt.exp(-alpha * tmat2) + beta\n",
    "    thetaj = pm.Deterministic(\"thetaj\", tt.clip(theta, 0, 1))\n",
    "\n",
    "    kij = pm.Binomial(\"kij\", p=thetaj, n=n, observed=k2)\n",
    "\n",
    "    # generate ppc\n",
    "    theta2 = tt.minimum(tt.exp(-alpha * tmat) + beta, 1.0)\n",
    "    rng = tt.shared_randomstreams.RandomStreams()\n",
    "    kij_ppc = pm.Deterministic(\"kij_ppc\", rng.binomial(n=n, p=theta2))\n",
    "\n",
    "    trace1_ = pm.sample(2000, tune=2000, init=\"advi\", target_accept=0.99)\n",
    "\n",
    "az.plot_trace(trace1_, var_names=[\"alpha\", \"beta\", \"thetaj\"], compact=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predict(trace1_[\"kij_ppc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Full individual differences\n",
    "\n",
    "\n",
    "$$ \\alpha_{i} \\sim \\text{Beta}(1,1)$$\n",
    "$$ \\beta_{i} \\sim \\text{Beta}(1,1)$$\n",
    "$$ \\theta_{ij} = \\text{min}(1,\\text{exp}(−\\alpha_{i} t_{j})+\\beta_{i})$$\n",
    "$$ k_{ij} \\sim \\text{Binomial}(\\theta_{ij},n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model2_:\n",
    "    alpha = pm.Beta(\n",
    "        \"alpha\",\n",
    "        alpha=1,\n",
    "        beta=1,\n",
    "        shape=(1, ns),\n",
    "        testval=np.asarray([[0.3, 0.3, 0.3, 0.5]]),\n",
    "    )\n",
    "    beta = pm.Beta(\n",
    "        \"beta\",\n",
    "        alpha=1,\n",
    "        beta=1,\n",
    "        shape=(1, ns),\n",
    "        testval=np.asarray([[0.25, 0.25, 0.25, 0.5]]),\n",
    "    )\n",
    "\n",
    "    theta = (tt.exp(-alpha[:, :ns2] * t2[:, None]) + beta[:, :ns2]).T\n",
    "    thetaj = pm.Deterministic(\"thetaj\", tt.clip(theta, 0, 1))\n",
    "\n",
    "    kij = pm.Binomial(\"kij\", p=thetaj, n=n, observed=k2)\n",
    "\n",
    "    # generate ppc\n",
    "    theta2 = tt.minimum((tt.exp(-alpha * t[:, None]) + beta).T, 1.0)\n",
    "    rng = tt.shared_randomstreams.RandomStreams()\n",
    "    kij_ppc = pm.Deterministic(\"kij_ppc\", rng.binomial(n=n, p=theta2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the difficulty of the model as explained above, here I do inference using fullrank_advi. The result seems quite comparable to JAGS actually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model2_:\n",
    "    # ADVI\n",
    "    s = theano.shared(pm.floatX(1))\n",
    "    inference = pm.FullRankADVI(cost_part_grad_scale=s)\n",
    "    # ADVI has nearly converged\n",
    "    inference.fit(n=20000)\n",
    "    # It is time to set `s` to zero\n",
    "    s.set_value(0)\n",
    "    approx = inference.fit(n=10000)\n",
    "    trace2_ = approx.sample(3000, include_transformed=True)\n",
    "\n",
    "    elbos1 = -inference.hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace2_, var_names=[\"alpha\", \"beta\"], compact=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproduce Fig 10.5\n",
    "def scatter_alpha_beta(trace):\n",
    "    from matplotlib.ticker import NullFormatter\n",
    "\n",
    "    nullfmt = NullFormatter()  # no labels\n",
    "    xtrace = trace[\"alpha\"][:1000].squeeze()\n",
    "    ytrace = trace[\"beta\"][:1000].squeeze()\n",
    "    ns_ = xtrace.shape[1]\n",
    "\n",
    "    # definitions for the axes\n",
    "    left, width = 0.1, 0.65\n",
    "    bottom, height = 0.1, 0.65\n",
    "    bottom_h = left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.2]\n",
    "    rect_histy = [left_h, bottom, 0.2, height]\n",
    "\n",
    "    # now determine limits by hand:\n",
    "    binwidth1 = 0.25\n",
    "\n",
    "    # start with a rectangular Figure\n",
    "    plt.figure(1, figsize=(8, 8))\n",
    "\n",
    "    cc = [\"C0\", \"C1\", \"C2\", \"C3\"]\n",
    "\n",
    "    for iss in np.arange(ns_):\n",
    "        x = xtrace[:, iss]\n",
    "        y = ytrace[:, iss]\n",
    "\n",
    "        axScatter = plt.axes(rect_scatter)\n",
    "        axScatter.set_xlim((0, 1))\n",
    "        axScatter.set_ylim((0, 1))\n",
    "\n",
    "        axHistx = plt.axes(rect_histx)\n",
    "        axHisty = plt.axes(rect_histy)\n",
    "\n",
    "        # no labels\n",
    "        axHistx.xaxis.set_major_formatter(nullfmt)\n",
    "        axHisty.yaxis.set_major_formatter(nullfmt)\n",
    "\n",
    "        # the scatter plot:\n",
    "        axScatter.scatter(x, y, c=cc[iss], alpha=0.1)\n",
    "        axScatter.set_xlabel(\"Decay Rate\", fontsize=18)\n",
    "        axScatter.set_ylabel(\"Baseline\", fontsize=18)\n",
    "\n",
    "        bins1 = np.linspace(0, 1, 50)\n",
    "        axHistx.hist(x, bins=bins1, color=cc[iss], alpha=0.5, normed=True)\n",
    "        bins2 = np.linspace(0, 1, 50)\n",
    "        axHisty.hist(\n",
    "            y,\n",
    "            bins=bins2,\n",
    "            color=cc[iss],\n",
    "            alpha=0.5,\n",
    "            normed=True,\n",
    "            orientation=\"horizontal\",\n",
    "        )\n",
    "\n",
    "        axHistx.set_xlim(axScatter.get_xlim())\n",
    "        axHisty.set_ylim(axScatter.get_ylim())\n",
    "\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, fitting using NUTS will give bias/different estimation compare to JAGS (same thing happens in [STAN](https://github.com/stan-dev/example-models/blob/master/Bayesian_Cognitive_Modeling/CaseStudies/MemoryRetention/Retention_2_Stan.R#L6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_alpha_beta(trace2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproduce Fig 10.3\n",
    "# The posterior predictive distribution for the model that assumes no individual\n",
    "# differences.\n",
    "plot_predict(trace2_[\"kij_ppc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Structured individual differences\n",
    "\n",
    "\n",
    "$$ \\mu_{\\alpha} \\sim \\text{Beta}(1,1)$$\n",
    "$$ \\lambda_{\\alpha} \\sim \\text{Gamma}(.001,.001)$$\n",
    "$$ \\mu_{\\beta} \\sim \\text{Beta}(1,1)$$\n",
    "$$ \\lambda_{\\beta} \\sim \\text{Gamma}(.001,.001)$$\n",
    "$$ \\alpha_{i} \\sim \\text{Gaussian}(\\mu_{\\alpha}, \\lambda_{\\alpha})_{\\mathcal I(0,1)} $$\n",
    "$$ \\beta_{i} \\sim \\text{Gaussian}(\\mu_{\\beta}, \\lambda_{\\beta})_{\\mathcal I(0,1)} $$\n",
    "$$ \\theta_{ij} = \\text{min}(1,\\text{exp}(−\\alpha_{i} t_{j})+\\beta_{i})$$\n",
    "$$ k_{ij} \\sim \\text{Binomial}(\\theta_{ij},n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BoundedNormal = pm.Bound(pm.Normal, lower=0, upper=1)\n",
    "with pm.Model() as model3:\n",
    "    mua = pm.Beta(\"mua\", alpha=1, beta=1)\n",
    "    lambdaa = pm.Gamma(\"lambdaa\", alpha=0.001, beta=0.001)\n",
    "    mub = pm.Beta(\"mub\", alpha=1, beta=1)\n",
    "    lambdab = pm.Gamma(\"lambdab\", alpha=0.001, beta=0.001)\n",
    "\n",
    "    alpha = BoundedNormal(\"alpha\", mu=mua, tau=lambdaa, shape=(1, ns))\n",
    "    beta = BoundedNormal(\"beta\", mu=mub, tau=lambdab, shape=(1, ns))\n",
    "\n",
    "    theta = (tt.exp(-alpha[:, :ns2] * t2[:, None]) + beta[:, :ns2]).T\n",
    "    thetaj = pm.Deterministic(\"thetaj\", tt.minimum(theta, 1))\n",
    "\n",
    "    kij = pm.Binomial(\"kij\", p=thetaj, n=n, observed=k2)\n",
    "\n",
    "    # generate ppc\n",
    "    theta2 = tt.minimum((tt.exp(-alpha * t[:, None]) + beta).T, 1.0)\n",
    "    rng = tt.shared_randomstreams.RandomStreams()\n",
    "    kij_ppc = pm.Deterministic(\"kij_ppc\", rng.binomial(n=n, p=theta2))\n",
    "\n",
    "    # step = pm.NUTS(target_accept=.99)\n",
    "    # trace3_ = pm.sample(1e4, tune=1000, init='advi', njobs=2)\n",
    "\n",
    "    # ADVI\n",
    "    s = theano.shared(pm.floatX(1))\n",
    "    inference = pm.FullRankADVI(cost_part_grad_scale=s)\n",
    "    # ADVI has nearly converged\n",
    "    inference.fit(n=20000)\n",
    "    # It is time to set `s` to zero\n",
    "    s.set_value(0)\n",
    "    approx = inference.fit(n=10000)\n",
    "    trace3_ = approx.sample(3000)\n",
    "\n",
    "    elbos1 = -inference.hist\n",
    "\n",
    "az.plot_trace(trace3_, var_names=[\"alpha\", \"beta\", \"mua\", \"mub\"], compact=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_alpha_beta(trace3_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predict(trace3_[\"kij_ppc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
